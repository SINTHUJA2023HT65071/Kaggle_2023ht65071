{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10612148,"sourceType":"datasetVersion","datasetId":6569862},{"sourceId":10612213,"sourceType":"datasetVersion","datasetId":6569909},{"sourceId":10612308,"sourceType":"datasetVersion","datasetId":6569984},{"sourceId":10612504,"sourceType":"datasetVersion","datasetId":6570114}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models.detection import maskrcnn_resnet50_fpn\nfrom torchvision.transforms import functional as F\nfrom pycocotools.coco import COCO\nimport numpy as np\nfrom PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T18:33:04.863026Z","iopub.execute_input":"2025-01-29T18:33:04.863350Z","iopub.status.idle":"2025-01-29T18:33:12.098878Z","shell.execute_reply.started":"2025-01-29T18:33:04.863320Z","shell.execute_reply":"2025-01-29T18:33:12.098033Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CocoDataset(Dataset):\n    def __init__(self, coco_annotations_file, image_dir, transforms=None):\n        self.coco = COCO(coco_annotations_file)\n        self.image_dir = image_dir\n        self.transforms = transforms\n        self.image_ids = list(self.coco.imgs.keys())\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, idx):\n        image_id = self.image_ids[idx]\n        image_info = self.coco.imgs[image_id]\n        image = Image.open(os.path.join(self.image_dir, image_info['file_name'])).convert(\"RGB\")\n\n        # Load annotations\n        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n        annotations = self.coco.loadAnns(ann_ids)\n\n        # Prepare target dictionary\n        masks = []\n        boxes = []\n        labels = []\n        for ann in annotations:\n            x_min, y_min, width, height = ann['bbox']\n\n            # Skip invalid bounding boxes\n            if width > 0 and height > 0:\n                masks.append(self.coco.annToMask(ann))\n                boxes.append([x_min, y_min, x_min + width, y_min + height])  # [x_min, y_min, x_max, y_max]\n                labels.append(ann['category_id'])\n\n        if len(boxes) == 0:\n            return None\n\n        masks = np.array(masks)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n\n        target = {\n            \"masks\": masks,\n            \"boxes\": boxes,\n            \"labels\": labels\n        }\n\n        image = F.to_tensor(image)\n\n        if self.transforms:\n            image, target = self.transforms(image, target)\n\n        return image, target\n\n\ndef collate_fn(batch):\n    # Ensure we use the built-in zip function\n    from builtins import zip\n\n    # Filter out None values from the batch\n    batch = [item for item in batch if item is not None]\n    if len(batch) == 0:\n        return [], []\n    return tuple(zip(*batch))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T18:34:11.406039Z","iopub.execute_input":"2025-01-29T18:34:11.406336Z","iopub.status.idle":"2025-01-29T18:34:11.414559Z","shell.execute_reply.started":"2025-01-29T18:34:11.406310Z","shell.execute_reply":"2025-01-29T18:34:11.413635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())\nprint(torch.cuda.device_count())\nprint(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T18:43:27.355631Z","iopub.execute_input":"2025-01-29T18:43:27.355969Z","iopub.status.idle":"2025-01-29T18:43:27.490639Z","shell.execute_reply.started":"2025-01-29T18:43:27.355941Z","shell.execute_reply":"2025-01-29T18:43:27.489931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T18:44:42.461173Z","iopub.execute_input":"2025-01-29T18:44:42.461480Z","iopub.status.idle":"2025-01-29T18:44:42.465257Z","shell.execute_reply.started":"2025-01-29T18:44:42.461454Z","shell.execute_reply":"2025-01-29T18:44:42.464272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CocoDataset(Dataset):\n    def __init__(self, coco_annotations_file, image_dir, transforms=None):\n        self.coco = COCO(coco_annotations_file)\n        self.image_dir = image_dir\n        self.transforms = transforms\n        self.image_ids = list(self.coco.imgs.keys())\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, idx):\n        image_id = self.image_ids[idx]\n        image_info = self.coco.imgs[image_id]\n        image = Image.open(os.path.join(self.image_dir, image_info['file_name'])).convert(\"RGB\")\n\n        # Load annotations\n        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n        annotations = self.coco.loadAnns(ann_ids)\n\n        # Prepare target dictionary\n        masks = []\n        boxes = []\n        labels = []\n        for ann in annotations:\n            x_min, y_min, width, height = ann['bbox']\n\n            # Skip invalid bounding boxes\n            if width > 0 and height > 0:\n                masks.append(self.coco.annToMask(ann))\n                boxes.append([x_min, y_min, x_min + width, y_min + height])  # [x_min, y_min, x_max, y_max]\n                labels.append(ann['category_id'])\n\n        if len(boxes) == 0:\n            return None\n\n        masks = np.array(masks)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n\n        target = {\n            \"masks\": masks,\n            \"boxes\": boxes,\n            \"labels\": labels\n        }\n\n        image = F.to_tensor(image)\n\n        if self.transforms:\n            image, target = self.transforms(image, target)\n\n        return image, target\n\n\ndef collate_fn(batch):\n    # Ensure we use the built-in zip function\n    from builtins import zip\n\n    # Filter out None values from the batch\n    batch = [item for item in batch if item is not None]\n    if len(batch) == 0:\n        return [], []\n    return tuple(zip(*batch))\n\n\n\ndef train_model():\n    train_dataset = CocoDataset(\n        coco_annotations_file='/kaggle/input/dataset/Dataset/train/_annotations.coco.json',\n        image_dir='/kaggle/input/dataset/Dataset/train',\n        transforms=None\n    )\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=4,\n        shuffle=True,\n        collate_fn=collate_fn  # Updated collate_fn\n    )\n\n    model = maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, 91)\n\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    model.to(device)\n\n    params = [p for p in model.parameters() if p.requires_grad]\n    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n\n    num_epochs = 30\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        for images, targets in train_loader:\n            if not images or not targets:  # Skip empty batches\n                continue\n\n            images = [image.to(device) for image in images]\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            optimizer.zero_grad()\n            loss_dict = model(images, targets)\n            losses = sum(loss for loss in loss_dict.values())\n\n            losses.backward()\n            optimizer.step()\n\n            total_loss += losses.item()\n\n        print(f\"Epoch #{epoch + 1} - Loss: {total_loss / len(train_loader)}\")\n        lr_scheduler.step()\n\n        torch.save(model.state_dict(), f'mask_rcnn_epoch_{epoch + 1}.pth')\n\n    torch.save(model.state_dict(), 'mask_rcnn_final.pth')\n\n\n\nif __name__ == \"__main__\":\n    train_model()\n    print('Training completed.')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T19:27:52.269177Z","iopub.execute_input":"2025-01-29T19:27:52.269485Z","iopub.status.idle":"2025-01-29T19:33:05.324767Z","shell.execute_reply.started":"2025-01-29T19:27:52.269459Z","shell.execute_reply":"2025-01-29T19:33:05.323825Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the checkpoint\nmodel = maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\nnum_classes = 91  # Number of classes in your dataset\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n\n# Load the checkpoint\ncheckpoint_path = \"/kaggle/working/mask_rcnn_final.pth\"  # Replace with the best checkpoint path\nmodel.load_state_dict(torch.load(checkpoint_path))\nmodel.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T19:34:49.942639Z","iopub.execute_input":"2025-01-29T19:34:49.942944Z","iopub.status.idle":"2025-01-29T19:34:50.822524Z","shell.execute_reply.started":"2025-01-29T19:34:49.942915Z","shell.execute_reply":"2025-01-29T19:34:50.821593Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = CocoDataset(\n    coco_annotations_file='/kaggle/input/dataset/Dataset/test/_annotations.coco.json',\n    image_dir='/kaggle/input/dataset/Dataset/test',\n    transforms=None\n)\n\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n\n# Evaluate the model\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\n\nall_outputs = []\nall_targets = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        images, targets = batch  # Unpack the batch\n        images = [image.to(device) for image in images]  # Move images to the device\n        outputs = model(images)  # Run the model on the images\n        all_outputs.extend(outputs)\n        all_targets.extend(targets)\n\n        # Process outputs and targets as needed\n        print(\"Outputs:\", outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T19:37:38.133248Z","iopub.execute_input":"2025-01-29T19:37:38.133605Z","iopub.status.idle":"2025-01-29T19:37:38.883435Z","shell.execute_reply.started":"2025-01-29T19:37:38.133576Z","shell.execute_reply":"2025-01-29T19:37:38.882536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport torch\nfrom PIL import Image\nimport torchvision.transforms.functional as F\nfrom google.colab.patches import cv2_imshow\nimport numpy as np\n\ndef process_video(video_path, model, output_path, threshold=0.5):\n    # Open the video\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        print(\"Error: Unable to open video file.\")\n        return\n\n    # Get video properties\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n\n    # Define the codec and create VideoWriter object\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Convert frame (BGR -> RGB) and preprocess\n        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        image_tensor = F.to_tensor(rgb_frame).unsqueeze(0).to(device)\n\n        # Model inference\n        with torch.no_grad():\n            outputs = model(image_tensor)[0]\n\n        # Filter predictions by threshold\n        if \"boxes\" not in outputs or \"scores\" not in outputs:\n            print(\"Error: Model outputs do not contain 'boxes' or 'scores'.\")\n            continue\n\n        boxes = outputs[\"boxes\"].cpu().numpy()\n        scores = outputs[\"scores\"].cpu().numpy()\n\n        # Filter boxes based on the threshold\n        filtered_boxes = [(boxes[i], scores[i]) for i in range(len(scores)) if scores[i] >= threshold]\n        if not filtered_boxes:\n            out.write(frame)\n            continue\n\n        # Sort boxes by score in descending order\n        filtered_boxes.sort(key=lambda x: x[1], reverse=True)\n\n        # Handle overlapping rectangles by keeping only the highest score\n        final_boxes = []\n        for box, score in filtered_boxes:\n            overlap = False\n            for final_box, _ in final_boxes:\n                x_min1, y_min1, x_max1, y_max1 = box\n                x_min2, y_min2, x_max2, y_max2 = final_box\n\n                # Check overlap\n                if not (x_max1 < x_min2 or x_min1 > x_max2 or y_max1 < y_min2 or y_min1 > y_max2):\n                    overlap = True\n                    break\n\n            if not overlap:\n                final_boxes.append((box, score))\n\n        # Draw diagonals for the final boxes\n        for box, score in final_boxes:\n            x_min, y_min, x_max, y_max = box\n\n            # Determine if the box is on the left or right side of the frame\n            box_center_x = (x_min + x_max) / 2\n\n            if box_center_x < width / 2:  # Left side: Draw `/` diagonal\n                cv2.line(frame, (int(x_min), int(y_max)), (int(x_max), int(y_min)), (255, 0, 0), 2)\n            else:  # Right side: Draw `\\` diagonal\n                cv2.line(frame, (int(x_min), int(y_min)), (int(x_max), int(y_max)), (255, 0, 0), 2)\n\n        # Write the frame to the output video\n        out.write(frame)\n\n        # Optionally display the video in real-time\n        cv2_imshow(frame)\n        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n            break\n\n    cap.release()\n    out.release()\n    cv2.destroyAllWindows()\n    print(f\"Output video saved at: {output_path}\")\n\nvideo_path = \"/kaggle/input/test-01/TestVideo.mp4\"  # Path to the input video\noutput_path = \"/kaggle/working/output_video_1.mp4\"  # Path to save the output video\nprocess_video(video_path, model, output_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T19:56:57.101451Z","iopub.execute_input":"2025-01-29T19:56:57.101810Z"}},"outputs":[],"execution_count":null}]}